{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多层感知机的从零开始实现\n",
    "\n",
    "我们已经从上一节里了解了多层感知机的原理。下面，我们一起来动手实现一个多层感知机。首先导入实现所需的包或模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import d2lzh as d2l\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import loss as gloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取和读取数据\n",
    "\n",
    "这里继续使用Fashion-MNIST数据集。我们将使用多层感知机对图像进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型参数\n",
    "\n",
    "我们在[“softmax回归的从零开始实现”](softmax-regression-scratch.ipynb)一节里已经介绍了，Fashion-MNIST数据集中图像形状为$28 \\times 28$，类别数为10。本节中我们依然使用长度为$28 \\times 28 = 784$的向量表示每一张图像。因此，输入个数为784，输出个数为10。实验中，我们设超参数隐藏单元个数为256。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens, num_hidden2 = 784, 10, 256, 64\n",
    "\n",
    "W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens))\n",
    "b1 = nd.zeros(num_hiddens)\n",
    "W2 = nd.random.normal(scale=0.01, shape=(num_hiddens, num_hidden2))\n",
    "b2 = nd.zeros(num_hidden2)\n",
    "W3 = nd.random.normal(scale=0.01, shape=(num_hidden2, num_outputs))\n",
    "b3 = nd.zeros(num_outputs)\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 笔记\n",
    "\n",
    "weights的初始化对训练的效果的影响。这是很重要学问相关的成熟的研究也有许多，如batch-norm、对参数进行预处理归一化这些操作、Xaier初始化，kaiming初始化等。\n",
    "\n",
    "在本练习中只是使用了普通将weights初始为标准高斯分布，对均值取值为**0附近的小随机数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义激活函数\n",
    "\n",
    "这里我们使用基础的`maximum`函数来实现ReLU，而非直接调用`relu`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "同softmax回归一样，我们通过`reshape`函数将每张原始图像改成长度为`num_inputs`的向量。然后我们实现上一节中多层感知机的计算表达式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H1 = relu(nd.dot(X, W1) + b1)\n",
    "    H2 = relu(nd.dot(H1, W2) + b2)\n",
    "    return nd.dot(H2, W3) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数\n",
    "\n",
    "为了得到更好的数值稳定性，我们直接使用Gluon提供的包括softmax运算和交叉熵损失计算的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "loss = gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "训练多层感知机的步骤和[“softmax回归的从零开始实现”](softmax-regression-scratch.ipynb)一节中训练softmax回归的步骤没什么区别。我们直接调用`d2lzh`包中的`train_ch3`函数，它的实现已经在[“softmax回归的从零开始实现”](softmax-regression-scratch.ipynb)一节里介绍过。我们在这里设超参数迭代周期数为5，学习率为0.5。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.3139, train acc 0.887, test acc 0.874\n",
      "epoch 2, loss 0.3087, train acc 0.888, test acc 0.879\n",
      "epoch 3, loss 0.3053, train acc 0.888, test acc 0.880\n",
      "epoch 4, loss 0.2970, train acc 0.891, test acc 0.882\n",
      "epoch 5, loss 0.2924, train acc 0.893, test acc 0.886\n",
      "epoch 6, loss 0.2879, train acc 0.895, test acc 0.878\n",
      "epoch 7, loss 0.2850, train acc 0.896, test acc 0.886\n",
      "epoch 8, loss 0.2837, train acc 0.895, test acc 0.886\n",
      "epoch 9, loss 0.2768, train acc 0.898, test acc 0.889\n",
      "epoch 10, loss 0.2724, train acc 0.899, test acc 0.880\n",
      "epoch 11, loss 0.2681, train acc 0.902, test acc 0.889\n",
      "epoch 12, loss 0.2641, train acc 0.903, test acc 0.885\n",
      "epoch 13, loss 0.2596, train acc 0.905, test acc 0.890\n",
      "epoch 14, loss 0.2595, train acc 0.905, test acc 0.885\n",
      "epoch 15, loss 0.2545, train acc 0.907, test acc 0.885\n",
      "epoch 16, loss 0.2501, train acc 0.907, test acc 0.884\n",
      "epoch 17, loss 0.2448, train acc 0.910, test acc 0.887\n",
      "epoch 18, loss 0.2481, train acc 0.909, test acc 0.889\n",
      "epoch 19, loss 0.2410, train acc 0.912, test acc 0.888\n",
      "epoch 20, loss 0.2381, train acc 0.913, test acc 0.889\n",
      "epoch 21, loss 0.2325, train acc 0.914, test acc 0.893\n",
      "epoch 22, loss 0.2289, train acc 0.916, test acc 0.894\n",
      "epoch 23, loss 0.2296, train acc 0.916, test acc 0.895\n",
      "epoch 24, loss 0.2239, train acc 0.917, test acc 0.894\n",
      "epoch 25, loss 0.2251, train acc 0.917, test acc 0.892\n",
      "epoch 26, loss 0.2180, train acc 0.920, test acc 0.892\n",
      "epoch 27, loss 0.2157, train acc 0.921, test acc 0.885\n",
      "epoch 28, loss 0.2140, train acc 0.921, test acc 0.892\n",
      "epoch 29, loss 0.2110, train acc 0.922, test acc 0.896\n",
      "epoch 30, loss 0.2111, train acc 0.922, test acc 0.896\n",
      "epoch 31, loss 0.2081, train acc 0.923, test acc 0.898\n",
      "epoch 32, loss 0.2024, train acc 0.926, test acc 0.892\n",
      "epoch 33, loss 0.2016, train acc 0.925, test acc 0.896\n",
      "epoch 34, loss 0.1965, train acc 0.928, test acc 0.878\n",
      "epoch 35, loss 0.1972, train acc 0.927, test acc 0.891\n",
      "epoch 36, loss 0.1949, train acc 0.929, test acc 0.899\n",
      "epoch 37, loss 0.1924, train acc 0.929, test acc 0.894\n",
      "epoch 38, loss 0.1871, train acc 0.932, test acc 0.896\n",
      "epoch 39, loss 0.1875, train acc 0.931, test acc 0.896\n",
      "epoch 40, loss 0.1863, train acc 0.931, test acc 0.897\n",
      "epoch 41, loss 0.1827, train acc 0.933, test acc 0.900\n",
      "epoch 42, loss 0.1817, train acc 0.934, test acc 0.897\n",
      "epoch 43, loss 0.1779, train acc 0.935, test acc 0.894\n",
      "epoch 44, loss 0.1757, train acc 0.935, test acc 0.893\n",
      "epoch 45, loss 0.1758, train acc 0.935, test acc 0.894\n",
      "epoch 46, loss 0.1714, train acc 0.938, test acc 0.897\n",
      "epoch 47, loss 0.1699, train acc 0.939, test acc 0.895\n",
      "epoch 48, loss 0.1684, train acc 0.938, test acc 0.899\n",
      "epoch 49, loss 0.1626, train acc 0.941, test acc 0.897\n",
      "epoch 50, loss 0.1674, train acc 0.939, test acc 0.837\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 50, 0.1\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 可以通过手动定义模型及其参数来实现简单的多层感知机。\n",
    "* 当多层感知机的层数较多时，本节的实现方法会显得较烦琐，例如在定义模型参数的时候。\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 改变超参数`num_hiddens`的值，看看对实验结果有什么影响。\n",
    "* 试着加入一个新的隐藏层，看看对实验结果有什么影响。\n",
    "\n",
    "\n",
    "\n",
    "## 扫码直达[讨论区](https://discuss.gluon.ai/t/topic/739)\n",
    "\n",
    "![](../img/qr_mlp-scratch.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
